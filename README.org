#+title: Home AI server

For more on this project's motivation and a setup guide, see the blog post:
https://shawnhoover.dev/notes/home-ai-server.html.

* Goals
- Easy-to-use, fully featured AI chat interface
- Access to one or more frontier models
- Multi-user
- Accessible on LAN only
- Pay as you go for all LLM interactions
- Installable on mobile as a progressive web app

* Implementation
- open-webui running in docker-compose
- Backed by AWS Bedrock foundation models and local ollama
- bedrock-access-gateway to access Bedrock chat models
- litellm API adapter to access a Bedrock embedding model
- nginx SSL termination with self-signed cert

* TODO
- [ ] Move deploy script to a CloudFormation stack
- [ ] Mount open-webui config.json
- [ ] Move secrets volumes to docker compose secrets
- [ ] Create backup strategy
- [ ] Set up auto-update
- [X] Document/automate checking out bedrock-access-gateway
- [X] Prefix AWS environment variables in .env

* Setup
When cloning this repo, you'll need to pull submodules to get Bedrock Access Gateway:

#+begin_src sh
git clone --recurse-submodules https://github.com/shoover/home-ai-server.git
#+end_src
